{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Runs Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Runs Test:\\n\")\n",
    "\n",
    "print(\"H0: The popularity of anime is random.\\n\")\n",
    "print(\"H1: The popularity of anime is not random.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = anime_data['popularity'].head(20)\n",
    "print(\"Populrity data:\\n\", subset)\n",
    "\n",
    "median = np.median(subset)\n",
    "print(\"Median:\",median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_num = 0\n",
    "n1 = 0  # Number of values below median\n",
    "n2 = 0  # Number of values above median\n",
    "above_median = None\n",
    "\n",
    "for popularity in subset:\n",
    "    if popularity > median:\n",
    "        if above_median is False or above_median is None:\n",
    "            runs_num += 1\n",
    "        above_median = True\n",
    "        n2 += 1\n",
    "    elif popularity < median:\n",
    "        if above_median is True or above_median is None:\n",
    "            runs_num += 1\n",
    "        above_median = False\n",
    "        n1 += 1\n",
    "\n",
    "print(\"Number of runs:\", runs_num)\n",
    "print(\"n1:\", n1)\n",
    "print(\"n2:\", n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The critical values from the table for n1 & n2 are 6 and 16.\")\n",
    "critic_below = 6\n",
    "critic_above = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if critic_below < runs_num < critic_above:\n",
    "    print(\"Accept H0. The popularity of animes is random.\")\n",
    "else:\n",
    "    print(\"Reject H0. The popularity of animes is not random.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Sample Sign Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"One sample sign test:\\n\")\n",
    "\n",
    "print(\"H0: The median rank is 8000.\\n\")\n",
    "print(\"H1: The median rank is not 8000.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 50\n",
    "subset = anime_data['ranked'].head(sample_size)\n",
    "median = np.median(subset)\n",
    "print(\"Median:\",median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 0  # Number of negative signs\n",
    "n2 = 0  # Number of positive signs\n",
    "\n",
    "for rank in subset:\n",
    "    if rank > median:\n",
    "        n2 += 1\n",
    "    elif rank < median:\n",
    "        n1 += 1\n",
    "\n",
    "print(\"Negative signs:\", n1)\n",
    "print(\"Positive signs:\", n2)\n",
    "n = n1 + n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_val = 1.645\n",
    "print(\"For two tailed test, 0.1 significance level, critical value is\",critic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sample_size < 26:\n",
    "    test_val = min(n1, n2)\n",
    "else:\n",
    "    test_val = ((min(n1,n2)+ 0.05) - (sample_size / 2))/ np.sqrt(sample_size / 2)\n",
    "print(\"Test value:\",test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_val <= critic_val:\n",
    "    print(\"Reject H0.\")\n",
    "else:\n",
    "    print(\"Accept H0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paired sample sign test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Paired sample sign test:\\n\")\n",
    "\n",
    "print(\"H0: There is difference in scores based on number of members.\\n\")\n",
    "print(\"H1: There is no difference in scores based on number of members.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreA = anime_data[anime_data['members'] <= 1000].head(10)['score'].tolist()\n",
    "scoreB = anime_data[anime_data['members'] > 1000].head(10)['score'].tolist()\n",
    "\n",
    "table_data = {\n",
    "    'Score A': scoreA,\n",
    "    'Score B': scoreB\n",
    "}\n",
    "\n",
    "table = pd.DataFrame(table_data)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = [a - b for a, b in zip(scoreA, scoreB)]\n",
    "\n",
    "signs = ['+' if diff > 0 else '-' if diff < 0 else '0' for diff in differences]\n",
    "\n",
    "differences_data = {\n",
    "    'Score A': scoreA,\n",
    "    'Score B': scoreB,\n",
    "    'Difference': differences,\n",
    "    'Sign': signs\n",
    "}\n",
    "\n",
    "differences_table = pd.DataFrame(differences_data)\n",
    "print(differences_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_positive = differences_table[differences_table['Sign'] == '+'].shape[0]\n",
    "num_negative = differences_table[differences_table['Sign'] == '-'].shape[0]\n",
    "\n",
    "print(\"Positive signs:\",num_positive)\n",
    "print(\"Negative signs:\",num_negative)\n",
    "\n",
    "test_val = min(num_positive,num_negative)\n",
    "print(\"Test value:\",test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = num_positive + num_negative\n",
    "print(\"For n =\",n,\"critical value is 1.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_val <= critic_val:\n",
    "    print(\"Reject H0. There is no difference in scores based on number of members.\")\n",
    "else:\n",
    "    print(\"Accept H0. There is difference in scores based on number of members.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon Rank Sum Test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wilcoxon Rank Sum Test:\\n\")\n",
    "\n",
    "print(\"H0: There is no significant difference between the distributions of scores based on members.\\n\")\n",
    "print(\"H1: There is significant difference between the distributions of scores based on members.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_val_positive = 1.645\n",
    "critic_val_negative = -1.645\n",
    "print(\"Critical value:\",critic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_a = anime_data[anime_data['members'] > 1000].head(10)[['score', 'members']]\n",
    "score_a['group'] = 'A'\n",
    "\n",
    "score_b = anime_data[anime_data['members'] <= 1000].head(10)[['score', 'members']]\n",
    "score_b['group'] = 'B'\n",
    "\n",
    "combined_data = pd.concat([score_a, score_b])\n",
    "\n",
    "combined_data = combined_data.sort_values(by='score')\n",
    "combined_data = combined_data.reset_index(drop=True)\n",
    "\n",
    "combined_data['rank'] = range(1, len(combined_data) + 1)\n",
    "\n",
    "print(combined_data[['score', 'group', 'rank']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_a = len(score_a)\n",
    "size_b = len(score_b)\n",
    "\n",
    "sum_ranks = combined_data.loc[combined_data['group'] == 'B', 'rank'].sum() if size_b < size_a else combined_data.loc[combined_data['group'] == 'A', 'rank'].sum()\n",
    "\n",
    "print(\"Size of Group A:\", size_a)\n",
    "print(\"Size of Group B:\", size_b)\n",
    "print(\"Sum of ranks for the group with the smaller sample size:\", sum_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = size_a\n",
    "n2 = size_b\n",
    "\n",
    "sigma = np.sqrt((n1*n2*(n1 + n2 + 1))/ 12)\n",
    "print(\"Sigma:\",sigma)\n",
    "\n",
    "u = (n1*(n1 + n2 + 1)) / 2\n",
    "print(\"u:\",u)\n",
    "\n",
    "R = sum_ranks\n",
    "print(\"R:\",sum_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_val = (R - u) / sigma\n",
    "print(\"Test value:\",test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if critic_val_negative < test_val < critic_val_positive:\n",
    "    print(\"Accept H0.\")\n",
    "else:\n",
    "    print(\"Reject H0.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wilcoxon Signed-rank Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Wilcoxon Signed-rank Test:\\n\")\n",
    "\n",
    "print(\"H0: There is difference in scores based on number of members.\\n\")\n",
    "print(\"H1: There is no difference in scores based on number of members.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10 # Sample size\n",
    "score_a = anime_data[anime_data['members'] > 1000].head(n)['score'].tolist()\n",
    "\n",
    "score_b = anime_data[anime_data['members'] <= 1000].head(n)['score'].tolist()\n",
    "\n",
    "differences = [a - b for a, b in zip(score_a, score_b)]\n",
    "absolute_diff = [abs(diff) for diff in differences]\n",
    "\n",
    "differences_data = {\n",
    "    'Score A': score_a,\n",
    "    'Score B': score_b,\n",
    "    'Difference': differences,\n",
    "    'Absolute Difference': absolute_diff\n",
    "}\n",
    "\n",
    "differences_table = pd.DataFrame(differences_data)\n",
    "\n",
    "differences_table['Rank'] = differences_table['Absolute Difference'].rank()\n",
    "differences_table['Signed Rank'] = differences_table.apply(lambda row: f\"+{int(row['Rank'])}\" if row['Difference'] > 0 else f\"-{int(row['Rank'])}\" if row['Difference'] < 0 else \"\", axis=1)\n",
    "\n",
    "print(differences_table[['Score A', 'Score B', 'Difference', 'Absolute Difference', 'Rank', 'Signed Rank']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_ranks = differences_table[differences_table['Signed Rank'].str.startswith('+')]\n",
    "negative_ranks = differences_table[differences_table['Signed Rank'].str.startswith('-')]\n",
    "\n",
    "sum_positive_ranks = positive_ranks['Rank'].sum()\n",
    "sum_negative_ranks = negative_ranks['Rank'].sum()\n",
    "\n",
    "print(\"Sum of Positive Ranks:\", sum_positive_ranks)\n",
    "print(\"Sum of Negative Ranks:\", sum_negative_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws = min(sum_positive_ranks,sum_negative_ranks)\n",
    "test_val = (Ws - ((n * (n + 1)) / 4)) / np.sqrt((n * (n + 1) * (2 * n + 1)) / 24)\n",
    "\n",
    "print(\"Test value:\",test_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_val_positive = 1.645\n",
    "critic_val_negative = -1.645\n",
    "print(\"Critical value:\",critic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if critic_val_negative < test_val < critic_val_positive:\n",
    "    print(\"Accept H0.\")\n",
    "else:\n",
    "    print(\"Reject H0.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
